{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arguments(hidden_size=2048, ffn_hidden_size=4096, num_layers=1, bias=True, return_bias=False, activation_fn='swiglu', moe_num_experts=4, moe_top_k=1, moe_capacity_factor=1, moe_normalize_expert_weights=None, moe_loss_weight=0.1, moe_zloss_weight=0.001, moe_jitter_eps=None, moe_lbl_in_fp32=False, moe_expert_choice=False, moe_expert_choice_grouped=False, moe_expert_model_parallelism=False, expert_parallel_group=None, moe_weight_parallelism=False, weight_parallel_group=None, pipeline_model_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, memory_optimized_mlp=False, mlp_type='mlp', mlp_impl='sparse', fp16=False, bf16=False, device=0, init_method=functools.partial(<function normal_ at 0x744347520c20>, mean=0.0, std=0.02), output_layer_init_method=functools.partial(<function normal_ at 0x744347520c20>, mean=0.0, std=0.02), uniform_expert_assignment=False, shared_expert=False, fc_cls=<class 'torch.nn.modules.linear.Linear'>, fc_kwargs={}, remat_act_fn=True, shared_expert_hidden_size=4096, shared_expert_weighted_sum=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def config_to_moe_args(d_model,hidden_ratio,device):\n",
    "    from megablocks.layers.arguments import Arguments as MoEArgs\n",
    "\n",
    "\n",
    "    kwargs = {\n",
    "        \"activation_fn\": \"swiglu\",\n",
    "        \"mlp_type\": \"mlp\",\n",
    "        \"hidden_size\": d_model,\n",
    "        \"ffn_hidden_size\": d_model*hidden_ratio,\n",
    "        \"moe_num_experts\": 4,\n",
    "        \"num_layers\": 1,\n",
    "        # Handled by FSDP (https://github.com/databricks/megablocks/issues/57#issuecomment-1854594483)\n",
    "        # \"moe_weight_parallelism\": False,\n",
    "        \"moe_expert_model_parallelism\": False,\n",
    "        \"moe_top_k\": 1,\n",
    "        # Handled by FSDP\n",
    "        \"bf16\": False,\n",
    "        \"fp16\": False,\n",
    "        \"return_bias\": False,\n",
    "    }\n",
    "\n",
    "    return MoEArgs(**kwargs)\n",
    "\n",
    "args = config_to_moe_args(2**11,2,\"cuda\")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67119104\n",
      "dMoE(\n",
      "  (router): LearnedRouter(\n",
      "    (layer): Linear(in_features=2048, out_features=4, bias=False)\n",
      "  )\n",
      "  (experts): ParallelDroplessMLP(\n",
      "    (mlp): SparseMLP()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from megablocks.layers.dmoe import dMoE\n",
    "\n",
    "dmoe = dMoE(args)\n",
    "\n",
    "# show the number of parameters\n",
    "print(sum(p.numel() for p in dmoe.parameters()))\n",
    "\n",
    "# show the number of parameters in the model\n",
    "print(dmoe)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
